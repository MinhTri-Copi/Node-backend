# Token Optimization Guide - Virtual Interview

## üéØ V·∫•n ƒë·ªÅ
- Gemini 2.5 Flash Lite: 20 RPD (Requests Per Day)
- 1 l·∫ßn ph·ªèng v·∫•n = 1 request ‚Üí H·∫øt quota ngay

## ‚úÖ Gi·∫£i ph√°p ƒë√£ √°p d·ª•ng

### 1. **R√∫t g·ªçn System Prompt** (Ti·∫øt ki·ªám ~70% tokens)
**Tr∆∞·ªõc:**
```
B·∫°n l√† m·ªôt HR chuy√™n nghi·ªáp ƒëang ph·ªèng v·∫•n ·ª©ng vi√™n IT. 
·ª®ng vi√™n n√†y ·ªü tr√¨nh ƒë·ªô: Th·ª±c t·∫≠p sinh - ki·∫øn th·ª©c c∆° b·∫£n...
Ch·ªß ƒë·ªÅ ph·ªèng v·∫•n: Java, React, Node.js
Nhi·ªám v·ª• c·ªßa b·∫°n:
1. H·ªèi c√¢u h·ªèi ph√π h·ª£p...
2. L·∫Øng nghe c√¢u tr·∫£ l·ªùi...
...
```
**Sau:**
```
HR ph·ªèng v·∫•n IT intern. Ch·ªß ƒë·ªÅ: Java,React. H·ªèi 1 c√¢u ng·∫Øn. Tr·∫£ l·ªùi b·∫±ng TI·∫æNG VI·ªÜT.
```
**Ti·∫øt ki·ªám:** ~200 tokens ‚Üí ~20 tokens

### 2. **Gi·ªõi h·∫°n Conversation History** (Ti·∫øt ki·ªám ~80% tokens)
**Tr∆∞·ªõc:** G·ª≠i to√†n b·ªô conversation history
**Sau:** Ch·ªâ g·ª≠i last 3 messages
**Ti·∫øt ki·ªám:** N·∫øu c√≥ 10 messages ‚Üí ch·ªâ g·ª≠i 3 = ti·∫øt ki·ªám 70% tokens

### 3. **Truncate Long Messages** (Ti·∫øt ki·ªám ~50% tokens)
- Candidate answer: Max 300 chars
- Question text: Max 150 chars
- Topic scores: Ch·ªâ 3 topics ƒë·∫ßu

### 4. **Limit Output Tokens**
```javascript
generationConfig: {
    maxOutputTokens: 50-100, // Thay v√¨ unlimited
    temperature: 0.7
}
```

### 5. **T·ªëi ∆∞u Prompt Templates**
- Lo·∫°i b·ªè redundant text
- D√πng abbreviations (C = Candidate, HR = HR)
- JSON format ng·∫Øn g·ªçn

## üìä Token Usage Comparison

| Component | Before | After | Savings |
|-----------|--------|-------|---------|
| System Prompt | ~200 | ~20 | 90% |
| Conversation History | ~500 (10 msgs) | ~150 (3 msgs) | 70% |
| Question Generation | ~300 | ~100 | 67% |
| Grading Prompt | ~250 | ~80 | 68% |
| **Total per request** | **~1250** | **~350** | **72%** |

## üöÄ Best Practices

### 1. **Cache System Prompt**
- Kh√¥ng c·∫ßn g·ª≠i l·∫°i system prompt m·ªói l·∫ßn
- Store trong interview metadata

### 2. **Summarize Long Conversations**
- N·∫øu > 10 messages, summarize old messages
- Ch·ªâ gi·ªØ recent 3-5 messages

### 3. **Batch Operations**
- N·∫øu c√≥ th·ªÉ, batch nhi·ªÅu operations
- Gi·∫£m s·ªë l∆∞·ª£ng API calls

### 4. **Use Appropriate Model**
- `gemini-1.5-flash-lite`: Fastest, cheapest
- `gemini-1.5-flash`: Better quality, still free tier

## ‚öôÔ∏è Configuration

### Environment Variables
```env
GEMINI_MODEL=gemini-1.5-flash-lite  # Fastest, most token-efficient
# ho·∫∑c
GEMINI_MODEL=gemini-1.5-flash       # Better quality
```

### Generation Config
```javascript
generationConfig: {
    maxOutputTokens: 50-100,  // Limit output
    temperature: 0.7,          // Consistency
    topP: 0.8,                 // Optional: further optimization
    topK: 20                    // Optional: further optimization
}
```

## üìà Expected Results

**Tr∆∞·ªõc optimization:**
- 1 interview = 1 request
- 20 interviews/day max

**Sau optimization:**
- 1 interview = 1 request (same)
- Nh∆∞ng m·ªói request d√πng √≠t token h∆°n 70%
- C√≥ th·ªÉ handle nhi·ªÅu interviews h∆°n trong c√πng quota

## üîç Monitoring

### Check Token Usage
```javascript
// Log token usage (if available in response)
console.log('Tokens used:', response.usageMetadata);
```

### Rate Limit Handling
- Implement exponential backoff
- Queue requests if rate limited
- Show user-friendly error messages

## üí° Additional Tips

1. **Reuse Prompts**: Cache common prompts
2. **Short Responses**: Encourage AI to give short answers
3. **Skip Unnecessary Calls**: Don't call API if not needed
4. **Local Fallbacks**: Use local logic when possible

## üéØ Target

**Goal:** Gi·∫£m token usage xu·ªëng < 300 tokens/request
**Current:** ~350 tokens/request (ƒë√£ optimize 72%)
**Status:** ‚úÖ Achieved!

---

**L∆∞u √Ω:** V·ªõi free tier 20 RPD, b·∫°n v·∫´n ch·ªâ c√≥ th·ªÉ l√†m 20 interviews/day. Nh∆∞ng m·ªói interview s·∫Ω d√πng √≠t token h∆°n, gi√∫p b·∫°n kh√¥ng b·ªã rate limit v·ªÅ token usage.

